\chapter{State of the Art}
\label{ch:literature}

To address performance challenges faced by modern runtime systems, vendors have invested considerable resources in adaptive optimization technology. Today, mainstream virtual machines come with sophisticated infrastructure for online profiling, run-time compilation, and feedback-directed optimization~\cite{Arnold05}. This chapter aims at providing an overview of the most commonly used techniques for adaptive optimizations systems.

\section{Profiling Techniques}
Motivated by the observation that most programs spend the majority of time in a small fraction of their code, virtual machines typically adopt {\em selective optimization} policies in order to focus their efforts on hot portions only. Indeed, optimization comes at a cost, and the expected performance gains from it should compensate for the overhead from collecting and processing profiling information and performing associated transformations.

\subsection*{Mechanisms for Collecting Profiles}
A key technical challenge for an adaptive optimization system is to collect accurate profile data while keeping the overhead low.

In order to collect coarse-grained information, such as the set of most frequently executed methods, two profiling mechanisms have emerged. Counter-based mechanisms associate counters with methods, and each counter is updated when the associated method is entered. A similar strategy can be adopted also to count how many times loop back edges are traversed. Sampling-based mechanisms instead periodically interrupt the program to inspect its state, for instance by walking the call stack, and they can incur a lower overhead when sampling is triggered by an external clock.

However, the most effective FDOs typically require finer-grained profiles, regarding, e.g., individual statements, objects, or paths taken in the control-flow graph of a function. Collecting such profiles with low overhead is a major challenge, especially online. Program instrumentation consists in injecting additional code in a running program, enabling the collection of a wide range of profiling data. Exhaustive instrumentation can be very expensive, and is typically combined with sampling techniques in order to affect only a limited percentage of the execution events. Several works have explored the trade-off between accuracy and performance in this scenario. In particular, Arnold and Ryder~\cite{Arnold01} described a technique that allows the system to turn instrumentation on and off at a fine granularity. A similar mechanism is used in~\cite{Zhuang06} to implement context-sensitive profiling in a JVM.

Indeed, the primary mechanism to reduce instrumentation overhead is to limit the time during which instrumented code executes~\cite{Arnold05}. Several VMs apply instrumentation to unoptimized code conly, turning it off when a method is recompiled. This approach has several advantages, but its main drawback is that it fails to capture changes in the dominant behavior after the early phases. Whaley~\cite{Whaley01} proposed a three-stage model in which instrumentation for fine-grained profiling is inserted in the second stage only. Multi-tier compilation systems, such as the one implemented in WebKit~\cite{Pizlo14}, may also insert instrumentation in later stages (i.e., in more optimized code as well).

The work on {\em vertical profiling} paper by Hauswirth \etal~\cite{Hauswirth04} sheds light on the need to perform profiling at all levels of the execution stack - including services provided by the runtime - for performance understanding. Indeed, techniques such as dynamic compilation and garbage collection influence program behavior in a way that makes correlation of performance to source code challenging.

Hardware performance monitors provided by specialized hardware in mainstream processors are an additional source of information that an adaptive optimizer may use. What makes them challenging to use in practice is the difficulty in mapping low-level collected data to high-level program constructs. Schneider \etal\ explored how to track them back to individual bytecode instructions in the Jikes RVM~\cite{Schneider07}.

\subsection*{Granularity of Profiling}

Program analysis and optimization techniques can be divided in two categories: {\em intra-procedural} techniques apply to one function at a time, while {\em inter-procedural} operate across function boundaries.

Examples of intra-procedural profiles are those collected by vertex, edge, and path profilers. A vertex profile counts the number of executions of each vertex (basic block) in a control-flow graph, while an edge profile counts the number of times each control-flow edge executes~\cite{Ball94}. A vertex profile can be used to guide a compiler in basic block placement. An edge profile always determine a vertex profile, and can be used to enable more powerful optimizations: for instance, Bond and McKinley~\cite{Bond05} investigated the impact of using continuous edge profiles to drive optimization in the Jikes RVM, with benefits in terms of, e.g., code reordering and register allocation. Path profiles provide finer-grained information, as they capture acyclic paths that are taken in the control-flow graph of a routine. The seminal work by Ball and Larus~\cite{Ball96} has spawned much research interest in the last 15 years in the design of novel techniques for collecting longer path profiles, as they can be used to guide several optimizations.

Inter-procedural profiles aim at capturing the interactions between functions in a program. For instance, context-sensitive profiles associate a metric with a sequence of procedures (a {\em calling context}) that are active during intervals of a program's execution~\cite{Ammons97}. In the context of statically compiled languages, expensive inter-procedural analyses can be used to attemp to prove invariants; in a managed environment, a profile can be used to speculate that invariants hold without proving them correct for all possible inputs. A virtual machine can thus apply many transformations speculatively, relying on ad-hoc invalidation mechanisms when the assumptions made during compilation no longer hold.

\section{Code Optimizers}
Mainstream runtime systems can resort to a large variety of techniques to generate executable code. A modern virtual machine typically relies on an interpreter (or a very fast baseline compiler) to execute a function the first time it is encountered, and then switches to a Just-In-Time (JIT) compilation system, usually with multiple optimization levels, to generate efficient code for a program's hot portions.

\subsection*{Interpretation}
Traditionally, an interpreter can be implemented using \mytt{switch}-based dispatcher that examines each instruction - typically after the source-level representation has been translated to a bytecode form - and processes it. Threading~\cite{Bell73} has been historically used to simplify dispatch logic in many implementations, nonetheless matching the performance of an optimizer compiler is an impossible goal.

%when resorting to JIT compilation is not possible or convenient (e.g., at early stages of a new language's development), 
Implementing a JIT is typically a large effort, as it affects a significant part of the existing language interpretation, and may not always be sustainable. Several researchers tried to come up with solutions to improve the interpretation process dynamically. Wurthinger \etal~\cite{Wurthinger12} proposed self-optimizing abstract syntax tree (AST) interpreters, which modify the AST representation of a program to incorporate type feedback in dynamic programming languages. Kalibera \etal\ adopted this idea in~\cite{Kalibera14}, in which they describe and evaluate a simple AST-based implementation of the R language running on an optimizing JVM that is competitive with - and usually faster than - other R implementations. Sullivan \etal in~\cite{Sullivan03} showed that the DynamoRIO dynamic binary optimizer~\cite{Bala00} can be used to remove much of the interpreted overhead from language implementations.

\subsection*{JIT Compilation}

JIT compilation is used by modern runtime to gain the benefits of both static compilation (e.g., generation of efficient code) and interpretation (e.g., access to run-time information). In his famous survey~\cite{Aycock03} Aycock dated the earliest published works on JIT compilation back to the '60s, while the seminal work on the Smalltalk-80 implementation~\cite{Deutsch84} epitomized the distinctive features of a modern JIT system.

\paragraph*{Multi-Tier JIT.} Sophisticated JIT compilers such as HotSpot Server~\cite{Paleczny01} can generate very high-quality code, but the time spent in the process ought to be compensated by the expected performance gains. Many modern runtimes implement multiple levels of JIT compilation, so that only the ``hottest'' portions of the code get compiled at the highest optimization level, while the number of optimizations applied to ``warm'' portions is typically smaller. For instance, the Jikes RVM~\cite{Alpern00} does not have an interpreter and uses a cost-benefit model feeded by call-stack samples to select between multiple levels of optimization.

\paragraph*{OSR Transitions.} Ideally, an adaptive optimization system should be able to generate a more optimized version of a function as soon as deemed necessary, and let the program run it. In the presence of long-running functions, however, it is not affordable for a runtime to wait for a less optimized function to complete, and redirect future invocations of it to the more optimized code. {\em On-Stack Replacement} (OSR) is thus used to replace a function while it executes, resuming execution in a different code version. OSR is a staple of modern runtimes, as it is used in optimization cycles to switch to faster code versions as soon as they become available, and - even more importantly - to perform deoptimization when a speculative assumption made for the running function at compilation time does not hold anymore.

\paragraph*{Trace-based JIT.} Trace-based JIT compilation has been proposed in~\cite{Gal09} to deal with the absence of concrete type information when compiling code for dynamic languages. Tracing JIT compilers can identify frequently executed sequences of instructions inside loops, and then generate specialized code for the dynamic types observed on each path through the loop. Guards are inserted in the optimized code to verify that types do not change across subsequent iterations: when this happens, execution leaves the trace through a side exit. Trace stitching can be used to concatenate sequences of optimized code at frequently taken side exits.

\paragraph*{Partial Evaluation.} Futamura~\cite{Futamura99} proposed partial evaluation to derive compiled code from an interpreter and a program's source code, and has extensively been studied for functional languages. Partial evaluation is used in Truffle/Graal~\cite{Wurthinger13} to perform aggressive optimizations assuming that the values of some variables do not change. For the interested reader, we refer to~\cite{Marr15} for a detailed discussion of the pros and cons of tracing JIT vs. partial evaluation.

\paragraph*{Basic Block Versioning.} Basic Block Versioning (BBV)~\cite{ChevalierBoisvert15} is a JIT compilation technique that generates type-specialized code for basic blocks lazily, interleaving execution and compilation as new type combinations are observed. BBV is simple to implement, can remove redundant type checks from critical code paths, and has recently been extended with simple inter-procedural optimizations. It might be thus an option for VM builders to implement a baseline JIT compiler.

\section{Feedback-Directed Optimization}
A fully automatic online Feedback-Directed Optimization (FDO) system is the key for continuous optimization in an adaptive runtime. As highligthed in~\cite{Arnold05}, FDO has several advantages: it can overcome limitations of static optimizers by exploiting run-time information that cannot be statically inferred; it enables the runtime to update its decision

%\section{Performance Profiling}
%\subsection{Means for Collecting Profiling Information}
%\subsection{Intra-Procedural Profiling Techniques}
%\subsection{Inter-Procedural Profiling Techniques}
%\section{Optimized Code Generation}
%\subsection{Profile-Guided Optimization}
%\subsection{Just-In-Time Compilation}
%\subsubsection{On-Stack Replacement}
%\subsubsection{Tracing JIT Compilation}
%\subsection{Other Related Work}

% Dynamic Binary Optimization, Self-Optimizing AST