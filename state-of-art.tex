\chapter{State of the Art}
\label{ch:literature}

To address performance challenges faced by modern runtime systems, vendors have invested considerable resources in adaptive optimization technology. Today, mainstream virtual machines come with sophisticated infrastructure for online profiling, run-time compilation, and feedback-directed optimization~\cite{Arnold05}. This chapter aims at providing an overview of the most commonly used techniques in adaptive optimizations systems. We will provide more detailed comparisons to the state of the art in the technical chapters of this dissertation.

\section{Profiling Techniques}
Motivated by the observation that most programs spend the majority of time in a small fraction of their code, virtual machines typically adopt {\em selective optimization} policies in order to focus their efforts on hot code portions only. Indeed, optimization comes at a cost, and the expected performance gains from it should compensate for the overhead from collecting and processing profiling information and performing associated transformations.

\subsection*{Mechanisms for Collecting Profiles}
A key technical challenge for an adaptive optimization system is to collect accurate profile data while keeping the overhead low.

In order to collect coarse-grained information, such as the set of most frequently executed methods, two profiling mechanisms have emerged. Counter-based mechanisms associate counters with methods, and each counter is updated when the associated method is entered. A similar strategy can be adopted also to count how many times each loop back edge is traversed. Sampling-based mechanisms instead periodically interrupt the program to inspect its state, for instance by walking the call stack, and they can incur a lower overhead than counter-based ones when sampling is triggered by an external clock.

However, the most effective feedback-directed optimizations typically require finer-grained profiles, regarding, e.g., individual statements, objects, or paths taken in the control-flow graph of a function. Collecting such profiles with low overhead is a major challenge, especially for use in online optimization. Program instrumentation consists in injecting additional code in a running program and enables the collection of a wide range of profiling data. Exhaustive instrumentation can be very expensive, and is typically combined with sampling techniques in order to affect only a limited percentage of the execution events. Several works have explored the trade-off between accuracy and performance in this scenario. In particular, Arnold and Ryder~\cite{Arnold01} described a technique that allows the system to turn instrumentation on and off at a fine granularity. A similar mechanism is used in~\cite{Zhuang06} to implement context-sensitive profiling in a JVM.

Indeed, the primary mechanism to reduce instrumentation overhead is to limit the time during which instrumented code executes~\cite{Arnold05}. Several VMs apply instrumentation to unoptimized code only, turning it off when a method is recompiled. This approach has several advantages, but its main drawback is that it fails to capture changes in the dominant behavior that happen after the early phases. Whaley~\cite{Whaley01} proposed a three-stage model in which instrumentation for fine-grained profiling is inserted in the second stage only. Multi-tier compilation systems, such as the one implemented in WebKit's JavaScript engine~\cite{Pizlo14}, may also insert instrumentation in later stages (i.e., in more optimized code as well).

The work on {\em vertical profiling} by Hauswirth \etal~\cite{Hauswirth04} sheds light on the need to perform profiling at all levels of the execution stack - including services provided by the runtime - for performance understanding. Indeed, techniques such as dynamic compilation and garbage collection influence program behavior in a way that makes correlation of performance to source code challenging.

Hardware performance monitors provided by specialized hardware in mainstream processors are an additional source of information that an adaptive optimizer may use. What makes them challenging to use in practice is the difficulty in mapping low-level collected data to high-level program constructs. Schneider \etal\ explored how to track them back to individual bytecode instructions in the Jikes RVM~\cite{Schneider07}.

\subsection*{Granularity of Profiling}

Program analysis and optimization techniques can be divided in two categories: {\em intra-procedural} techniques apply to one function at a time, while {\em inter-procedural} ones operate across function boundaries.

Examples of intra-procedural profiles are those collected by vertex, edge, and path profilers. A vertex profile counts the number of executions of each vertex (basic block) in a control-flow graph, while an edge profile counts the number of times each control-flow edge executes~\cite{Ball94}. A vertex profile can be used to guide a compiler in basic block placement. An edge profile always determine a vertex profile, and can be used to enable more powerful optimizations: for instance, Bond and McKinley~\cite{Bond05} investigated the impact of using continuous edge profiles to drive optimization in the Jikes RVM, with benefits in terms of, e.g., code reordering and register allocation. Path profiles provide finer-grained information, as they can capture acyclic paths that are taken in the control-flow graph of a routine. The seminal work by Ball and Larus~\cite{Ball96} has spawned much research interest in the last 15 years in the design of novel techniques for collecting longer path profiles, as they can be used to guide several optimizations.

Inter-procedural profiles aim at capturing the interactions between functions in a program. For instance, context-sensitive profiles associate a metric with a sequence of procedures (a {\em calling context}) that are active during intervals of a program's execution~\cite{Ammons97}. In the context of statically compiled languages, expensive inter-procedural analyses can be used to attemp to prove invariants; in a managed environment, a profile can be used to speculate that invariants hold without proving them correct for all possible inputs. A virtual machine can thus apply many transformations speculatively, relying on ad-hoc invalidation mechanisms when execution diverges from the assumptions made during compilation.

\section{Code Optimizers}
Mainstream runtime systems can resort to a large variety of techniques to generate executable code. A modern virtual machine typically relies on an interpreter (or a very fast baseline compiler) to execute a function the first time it is encountered, and then resorts to a Just-In-Time (JIT) compilation system, usually equipped with multiple optimization levels, to generate efficient code for a program's hot portions.

\subsection*{Interpretation}
Traditionally, an interpreter can be implemented using \mytt{switch}-based dispatcher that examines each instruction - typically after the source-level representation has been translated to a bytecode form - and processes it. Threading~\cite{Bell73} has been historically used to simplify dispatch logic in many implementations. Nonetheless, matching the performance of optimized compiled code remains an impossible goal.

%when resorting to JIT compilation is not possible or convenient (e.g., at early stages of a new language's development), 
Implementing a JIT is typically a large effort, as it affects a significant part of the existing language interpretation, and may not always be sustainable. Several researchers tried to come up with solutions to improve the interpretation process dynamically. Wurthinger \etal~\cite{Wurthinger12} proposed self-optimizing abstract syntax tree (AST) interpreters, which modify the AST representation of a program to incorporate type feedback in dynamic programming languages. Kalibera \etal\ adopted this idea in~\cite{Kalibera14}, in which they describe and evaluate a simple AST-based implementation of the R language running on an optimizing JVM that is competitive with - and usually faster than - other R implementations. Sullivan \etal\ in~\cite{Sullivan03} showed that the DynamoRIO dynamic binary optimizer~\cite{Bala00} can be used to remove much of the interpreted overhead from language implementations.

\subsection*{JIT Compilation}

JIT compilation is used by modern runtime to gain the benefits of both static compilation (e.g., generation of efficient code) and interpretation (e.g., access to run-time information). In his famous survey~\cite{Aycock03}, Aycock dated the earliest published works on JIT compilation back to the '60s, while the seminal work on the Smalltalk-80 implementation~\cite{Deutsch84} epitomized the distinctive features of a modern JIT system.

\paragraph*{Multi-Level JIT.} Sophisticated JIT compilers such as HotSpot Server~\cite{Paleczny01} can generate very high-quality code, but the time spent in the compilation process ought to be compensated by the expected performance gains. Many modern runtimes implement multiple levels of JIT compilation, so that only the ``hottest'' portions of the code get compiled at the highest optimization level, while the number of optimizations applied to ``warm'' portions is typically smaller. For instance, the Jikes RVM~\cite{Alpern00} does not have an interpreter and uses a cost-benefit model feeded by call-stack samples to select between multiple levels of optimization.

\paragraph*{OSR Transitions.} Ideally, an adaptive optimization system should be able to generate a more optimized version of a function as soon as deemed necessary, and let the program run it. In the presence of long-running functions, however, it is not affordable for a runtime to wait for a less optimized function to complete, redirecting only future invocations of it to the more optimized code. {\em On-Stack Replacement} (OSR) is thus used to replace a function while it executes, resuming execution in a different code version. OSR is a staple of modern runtimes, as it is used in optimization cycles to switch to faster code versions as soon as they become available, and to perform deoptimization when a speculative assumption made for the running function at compilation time does not hold anymore.

\paragraph*{Trace-based JIT.} Trace-based JIT compilation has been proposed in~\cite{Gal09} to deal with the absence of concrete type information when compiling code for dynamic languages. Tracing JIT compilers can identify frequently executed sequences of instructions inside loops, and then generate specialized code for the dynamic types observed on each path through the loop. Guards are inserted in the optimized code to verify that types do not change across subsequent iterations: when this happens, execution leaves the trace through a side exit. Trace stitching can be used to concatenate sequences of optimized code at frequently taken side exits.

\paragraph*{Partial Evaluation.} Futamura~\cite{Futamura99} proposed partial evaluation to derive compiled code from an interpreter and a program's source code, and has extensively been studied for functional languages. Partial evaluation is used in Truffle/Graal~\cite{Wurthinger13} to perform aggressive optimizations assuming that the values of some variables do not change. For the interested reader, we refer to~\cite{Marr15} for a detailed discussion of the pros and cons of tracing JIT vs. partial evaluation.

\paragraph*{Basic Block Versioning.} Basic Block Versioning (BBV)~\cite{ChevalierBoisvert15} is a JIT compilation technique that generates type-specialized code for basic blocks lazily, interleaving execution and compilation as new type combinations are observed. BBV is simple to implement, can remove redundant type checks from critical code paths, and has recently been extended with simple but effective inter-procedural optimizations~\cite{ChevalierBoisvert16}. It might be thus an option for VM builders to implement a baseline JIT compiler.

\section{Feedback-Directed Optimization}
A fully automatic online Feedback-Directed Optimization (FDO) system is the key for continuous optimization in an adaptive runtime. As highligthed in~\cite{Arnold05}, FDO has several advantages: for instance, it can overcome limitations of static optimizers by exploiting run-time information that cannot be statically inferred, and it allows the runtime to change a decision if and when conditions change.

The universe of FDO techniques is rather large. For this reason, in the remainder of this section, we will present in more detail those techniques to which the ideas presented in this thesis are more applicable, and briefly mention the others.

\paragraph*{Inlining.} Replacing a call site with the code of the function it invokes is one of the most widely employed optimization techniques. Inlining can be particularly effective in the context of object-oriented languages, but performing it too aggressively would place a burden on compilation time and code bloat. The implementations of the SELF language~\cite{Holzle92,Holzle96} introduced progressively sophisticated techniques for effective dynamic inlining: in particular, they augmented inlining with type feedback based on types profiled for the receivers.

Lessons learned from the devirtualization of function calls in statically compiled object oriented languages (e.g., ~\cite{Bacon96,Dean96}) have been precious to implement inlining in the presence of dynamic class loading. For instance, HotSpot performs guarded inlining when class hierarchy analysis suggests a likely monomorphic call site.

\paragraph*{Multiversioning and Specialization.} A compiler may decide to generate multiple implementations of a code sequence, and let the program choose the best one at run time. In a dynamic setting, versions can be generated using run-time profiling information: the guarded inlining example discussed above is indeed a form of multiversioning. To divert execution to a safe code version, a compiler can either add a slow path in the code - possibly separated from the fast path to avoid polluting the results of data-flow analyses - or trigger an OSR when a guard fails. The latter approach has been used for instance in SELF-91 to support deferred compilation~\cite{Chambers91}; HotSpot and Jikes RVM implement it in a similar manner.

Specialization is a form of multi-versioning that speculates on run-time facts, and is thus closely related to partial evaluation. When a type inference engine cannot infer precise information for a function, type-based JIT specialization can be used to generate specialized copies of the function body~\cite{Cooper92} by speculating on each argument's type. Value-based JIT specialization~\cite{Santos13} creates specialized function bodies based on the run-time values for a subset of the arguments, and can be effective for functions that are either called only once, or repeatedly invoked with the same parameters.

\paragraph*{Instruction Scheduling.} Instruction scheduling techniques attempt to maximize the flow of instructions through a processor's pipeline by reordering them on the basis of observed profiles. Extensively studied in static code generation for superscalar processors, these techniques have been explored for JIT compilers as well~\cite{Arnold05, Touati14}.

\paragraph*{Other Relevant Techniques.} Unless otherwise stated, we refer the reader to~\cite{Arnold05} for the techniques mentioned here. Polymorphic Inline Caches (PICs) can be used to perform dynamic dispatching of methods based on previously seen cases. PICs are also used by modern JavaScript engines to optimize object property access~\cite{ChevalierBoisvert16}.

\noindent Escape analysis~\cite{Choi99} can permit stack placement of an object: it is typically enabled by other transformations, such as speculative inlining of currently monomorphic call sites, and paves the way to further optimization such as the promotion of an object's fields from memory to registers.

Code positioning techniques can be used to improve branch prediction and maximize instruction-cache locality by rearranging the instructions in the code. Production VMs can also dynamically adjust the size of their heap depending on the memory allocation requests of the running application. Garbage collection is typically triggered at {\em safepoints}, at which all threads are suspended and also other tasks such as deoptimization and code cache flushing can be performed.

% MISSING: profile-guided optimization (PGO) for statically compiled code!